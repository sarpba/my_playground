{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Először telepítsük a szükséges csomagokat\n",
        "!pip install pyannote.audio\n",
        "!pip install soundfile\n",
        "!pip install pydub\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ],
      "metadata": {
        "id": "b9hFX8MOpqvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "itzSW7GtwRQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CPU version"
      ],
      "metadata": {
        "id": "zux7fcY5HtvP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from pyannote.audio import Pipeline\n",
        "from pyannote.core import Segment\n",
        "import soundfile as sf\n",
        "import numpy as np\n",
        "\n",
        "# Initialize the speaker diarization pipeline\n",
        "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization\")\n",
        "\n",
        "# Apply the pipeline to the audio file\n",
        "diarization = pipeline(\"/content/input.wav\")\n",
        "\n",
        "# Load the audio file\n",
        "audio, sr = sf.read('/content/input.wav')\n",
        "\n",
        "# Create masks for each speaker\n",
        "speaker1_audio = np.zeros_like(audio)\n",
        "speaker2_audio = np.zeros_like(audio)\n",
        "\n",
        "# Iterate over the diarization result and separate speakers\n",
        "for segment, _, speaker in diarization.itertracks(yield_label=True):\n",
        "    start = int(segment.start * sr)\n",
        "    end = int(segment.end * sr)\n",
        "    if speaker == 'SPEAKER_00':\n",
        "        speaker1_audio[start:end] = audio[start:end]\n",
        "    elif speaker == 'SPEAKER_01':\n",
        "        speaker2_audio[start:end] = audio[start:end]\n",
        "\n",
        "# Save the separated audio files to disk\n",
        "sf.write('speaker1.wav', speaker1_audio, sr)\n",
        "sf.write('speaker2.wav', speaker2_audio, sr)\n"
      ],
      "metadata": {
        "id": "Rk1RxPmyvqfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPU version"
      ],
      "metadata": {
        "id": "JL-jcj9KHrHG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from pyannote.audio import Pipeline\n",
        "from pyannote.core import Segment\n",
        "import soundfile as sf\n",
        "import numpy as np\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize the speaker diarization pipeline\n",
        "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization\", use_auth_token=\"YOUR_HF_TOKEN\").to(device)\n",
        "\n",
        "# Apply the pipeline to the audio file\n",
        "diarization = pipeline(\"/content/orban.wav\")\n",
        "\n",
        "# Load the audio file\n",
        "audio, sr = sf.read('/content/orban.wav')\n",
        "\n",
        "# Create masks for each speaker\n",
        "speaker1_audio = np.zeros_like(audio)\n",
        "speaker2_audio = np.zeros_like(audio)\n",
        "\n",
        "# Iterate over the diarization result and separate speakers\n",
        "for segment, _, speaker in diarization.itertracks(yield_label=True):\n",
        "    start = int(segment.start * sr)\n",
        "    end = int(segment.end * sr)\n",
        "    if speaker == 'SPEAKER_00':\n",
        "        speaker1_audio[start:end] = audio[start:end]\n",
        "    elif speaker == 'SPEAKER_01':\n",
        "        speaker2_audio[start:end] = audio[start:end]\n",
        "\n",
        "# Save the separated audio files to disk\n",
        "sf.write('speaker1.wav', speaker1_audio, sr)\n",
        "sf.write('speaker2.wav', speaker2_audio, sr)\n"
      ],
      "metadata": {
        "id": "QBWfCfdvHV3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPU batch recursive"
      ],
      "metadata": {
        "id": "cngLZ_eOK8wU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from pyannote.audio import Pipeline\n",
        "from pyannote.core import Segment\n",
        "import soundfile as sf\n",
        "import numpy as np\n",
        "import os\n",
        "import tempfile\n",
        "from pydub import AudioSegment\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize the speaker diarization pipeline\n",
        "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization\", use_auth_token=\"YOUR_HF_TOKEN\").to(device)\n",
        "\n",
        "def convert_to_wav(mp3_file):\n",
        "    # Load MP3 file\n",
        "    audio = AudioSegment.from_mp3(mp3_file)\n",
        "    # Convert to WAV\n",
        "    wav_file = tempfile.NamedTemporaryFile(suffix='.wav', delete=False)\n",
        "    wav_file.close()  # Close to release handle\n",
        "    audio.export(wav_file.name, format=\"wav\")\n",
        "    return wav_file.name\n",
        "\n",
        "def process_audio_file(file_path):\n",
        "    if file_path.lower().endswith(\".mp3\"):\n",
        "        # Convert MP3 to WAV\n",
        "        file_path = convert_to_wav(file_path)\n",
        "\n",
        "    # Apply the pipeline to the audio file\n",
        "    diarization = pipeline(file_path)\n",
        "\n",
        "    # Load the audio file\n",
        "    audio, sr = sf.read(file_path)\n",
        "\n",
        "    # Create masks for each speaker\n",
        "    speaker1_audio = np.zeros_like(audio)\n",
        "    speaker2_audio = np.zeros_like(audio)\n",
        "\n",
        "    # Iterate over the diarization result and separate speakers\n",
        "    for segment, _, speaker in diarization.itertracks(yield_label=True):\n",
        "        start = int(segment.start * sr)\n",
        "        end = int(segment.end * sr)\n",
        "        if speaker == 'SPEAKER_00':\n",
        "            speaker1_audio[start:end] = audio[start:end]\n",
        "        elif speaker == 'SPEAKER_01':\n",
        "            speaker2_audio[start:end] = audio[start:end]\n",
        "\n",
        "    # Save the separated audio files to disk\n",
        "    base_filename = os.path.splitext(os.path.basename(file_path))[0]\n",
        "    sf.write(f'{base_filename}_speaker1.wav', speaker1_audio, sr)\n",
        "    sf.write(f'{base_filename}_speaker2.wav', speaker2_audio, sr)\n",
        "\n",
        "    # Clean up temporary WAV file if MP3 was converted\n",
        "    if file_path.lower().endswith(\".mp3\"):\n",
        "        os.remove(file_path)\n",
        "\n",
        "def process_directory(directory):\n",
        "    for root, _, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            process_audio_file(file_path)\n",
        "\n",
        "# Directory containing the audio files\n",
        "directory = '/content/audio_files'\n",
        "\n",
        "# Process all audio files in the directory\n",
        "process_directory(directory)\n",
        "\n"
      ],
      "metadata": {
        "id": "4--EF36gIYdK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}